{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci konwolucyjne\n",
    "\n",
    "Zadanie dla Państwa na te ćwiczenia to implementacja funkcji konwolucji oraz max pooling dla obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting\n",
    "%matplotlib inline\n",
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20c8bba82b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data loading\n",
    "cifar_sample = np.load('./resources/cifar_sample.npy')\n",
    "# get a first random image\n",
    "np_image = cifar_sample[0]\n",
    "# this should plot a blurry frog\n",
    "plt.imshow(np_image.transpose(1,2,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wzory na rozmiary\n",
    "**Pytanie 1**: Jaki będzie rozmiar obrazka na wyjściu konwolucji/poolingu przy parametrach poniżej.  \n",
    "**Uwaga**: zarówno we wzorach jak i w kodzie używana jest torchowa konwencja *channel first*.\n",
    "\n",
    "Stride: $ \\hspace{95px} S $  \n",
    "Padding: $ \\hspace{80px} P $  \n",
    "Obrazek wejściowy: $ \\hspace{12px} C_i \\times H_i \\times W_i$  \n",
    "Filtry: $ \\hspace{100px} K \\times C_f \\times F \\times F $  \n",
    "\n",
    "Gdzie: $C_i$ to liczba kanału obrazu wejściowego, $H_i, W_i$ to odpowiednio wysokość i szerokość obrazu wejściowego. $K$ to liczba filtrów, $C_f$ liczba kanałów w każdym filtrze, $F$ to zarówno wysokość jak i szerokość filtra (rozważamy tylko filtry kwadratowe).\n",
    "\n",
    "Obrazek wyjściowy: $ \\hspace{15px} C_o \\times H_o \\times W_o $  \n",
    "\n",
    "\n",
    "$ \\hspace{140px} C_o = \\text{???} $  \n",
    "\n",
    "$ \\hspace{140px} H_o = \\text{???} $  \n",
    "\n",
    "$ \\hspace{140px} W_o = \\text{???} $  \n",
    "\n",
    "**Pytanie 2**: Ile wag (floatów) ma taka warstwa konwolucyja?   \n",
    "\n",
    "\n",
    "### Wizualna pomoc do konwolucji\n",
    "[Źródło](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "<img src=\"./resources/cnn.gif\"></img>\n",
    "\n",
    "### Zadanie 1:  Konwolucja (5 pkt.)\n",
    "Zadaniem jest zaimplementowanie funkcji konwolucji i poolingu dla obrazka 2D. Implementacja nie musi być optymalna pod względem złożoności czasowej (tzn. można/zaleca się używać pętli). \n",
    "\n",
    "Warunkiem zaliczenia zadania jest przejście komórek testowych dla konwolucji i poolingu. W razie problemów polecam zacząć od poolingu, który jest podobny do konwolucji, ale mniej skomplikowany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad as pad\n",
    "import torchvision.transforms.functional as TF\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def convolution(image: torch.tensor, \n",
    "                filters: torch.tensor, \n",
    "                bias: torch.tensor, \n",
    "                stride: int = 1, \n",
    "                padding: int = 1):\n",
    "    \"\"\"\n",
    "    :param image: torch.Tensor \n",
    "        Input image of shape (C, H, W)\n",
    "    :param filters: torch.Tensor \n",
    "        Filters to use in convolution of shape (K, C, F, F)\n",
    "    :param bias: torch.Tensor \n",
    "        Bias vector of shape (K,)\n",
    "    :param stride: int\n",
    "        Stride to use in convolution\n",
    "    :param padding: int\n",
    "       Zero-padding to add on all sides of the image \n",
    "    \"\"\"\n",
    "    # get image dimensions\n",
    "    # torch.Size([3, 32, 32])\n",
    "    img_channels, img_height, img_width = image.shape \n",
    "    # torch.Size([2, 3, 3, 3])\n",
    "    n_filters, filter_channels, filter_size, filter_size = filters.shape \n",
    "    # calculate the dimensions of the output image 30x30\n",
    "    out_height = int(((img_height - filter_size + 2 * padding) / stride) + 1)\n",
    "    out_width = int(((img_width - filter_size + 2 * padding) / stride) + 1)\n",
    "    image_orig = image\n",
    "    # torch.Size([3, 34, 34])\n",
    "    image_padded = pad(image_orig, (0, 0, 2, 1))\n",
    "    img_channels2, img_height2, img_width2 = image_padded.shape \n",
    "    \n",
    "    # Iterate through image\n",
    "    output = torch.zeros((n_filters,out_height,out_width))\n",
    "    for i in range(n_filters):\n",
    "        for c in range(img_channels):\n",
    "            for j in range(0, img_height - filter_size + 1, stride):\n",
    "                for x in range(0, img_width - filter_size + 1, stride):\n",
    "                    img_crop = image_padded[:, j:j+filter_size, x:x+filter_size]\n",
    "                    filter_tab = filters[i,:,:,:] \n",
    "                    output[:, j // stride, x // stride] = torch.multiply(filter_tab,img_crop).sum() + bias[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.3151, -1.7664, -1.9380,  ..., -0.8490, -0.8942, -0.9625],\n",
      "          [-2.3003, -2.5802, -2.5087,  ..., -1.5726, -1.5703, -1.4782],\n",
      "          [-2.4601, -2.6254, -2.1577,  ..., -1.6001, -1.8023, -1.7849],\n",
      "          ...,\n",
      "          [-1.2501, -1.8537, -1.8248,  ..., -2.1004, -0.1001, -0.4254],\n",
      "          [-1.2675, -1.8226, -1.9734,  ..., -1.9293, -0.2751, -1.0006],\n",
      "          [-0.7256, -0.9603, -1.2841,  ..., -1.9715, -0.3777, -0.8762]],\n",
      "\n",
      "         [[ 0.1349,  0.0311,  0.0707,  ...,  1.0591,  1.0485,  0.9024],\n",
      "          [ 0.0813, -0.1462,  0.0208,  ...,  0.8627,  0.7392,  0.5217],\n",
      "          [ 0.0547,  0.1113,  0.0994,  ...,  0.7438,  0.6398,  0.6095],\n",
      "          ...,\n",
      "          [ 1.0384,  1.0881,  1.0636,  ...,  1.7800,  1.0275,  0.5600],\n",
      "          [ 1.2976,  1.3406,  1.3545,  ...,  1.3998,  1.1076,  0.4607],\n",
      "          [ 0.9797,  1.1844,  1.4420,  ...,  0.9888,  1.4394,  0.7136]]]])\n",
      "----------------------------------------------------------------------------------\n",
      "tensor([[[-0.1109, -0.0964,  0.0060,  ..., -0.0489, -0.1050, -0.0579],\n",
      "         [ 0.6137,  0.6757,  0.7745,  ...,  1.6136,  1.6834,  1.6435],\n",
      "         [ 0.1349,  0.0311,  0.0707,  ...,  1.0591,  1.0485,  0.9024],\n",
      "         ...,\n",
      "         [ 0.6328,  0.4003,  0.2457,  ...,  1.1913,  1.4147,  1.8125],\n",
      "         [ 0.8058,  0.7780,  0.6643,  ...,  1.4599,  1.1650,  1.2843],\n",
      "         [ 1.0384,  1.0881,  1.0636,  ...,  1.7800,  1.0275,  0.5600]],\n",
      "\n",
      "        [[-0.1109, -0.0964,  0.0060,  ..., -0.0489, -0.1050, -0.0579],\n",
      "         [ 0.6137,  0.6757,  0.7745,  ...,  1.6136,  1.6834,  1.6435],\n",
      "         [ 0.1349,  0.0311,  0.0707,  ...,  1.0591,  1.0485,  0.9024],\n",
      "         ...,\n",
      "         [ 0.6328,  0.4003,  0.2457,  ...,  1.1913,  1.4147,  1.8125],\n",
      "         [ 0.8058,  0.7780,  0.6643,  ...,  1.4599,  1.1650,  1.2843],\n",
      "         [ 1.0384,  1.0881,  1.0636,  ...,  1.7800,  1.0275,  0.5600]]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-8143ba8b3a2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mout_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convolution Test\n",
    "\n",
    "# cast the frog to tensor\n",
    "image = torch.tensor(np_image)\n",
    "# preapre parameters for testing\n",
    "paddings = [0, 1, 2, 3]\n",
    "strides = [1, 2, 3, 4]\n",
    "filters = [(torch.randn((2,3,3,3)), torch.randn((2))),\n",
    "           (torch.randn((2,3,5,5)), torch.randn((2))),\n",
    "           (torch.randn((5,3,1,1)), torch.randn((5)))]\n",
    "\n",
    "# test all combinations\n",
    "for (filt, bias), stride, padding in product(filters, strides, paddings):\n",
    "    # your convolution\n",
    "    # PyTorch equivalent\n",
    "    out_torch = torch.conv2d(input=image.unsqueeze(0), weight=filt, bias=bias, padding=padding, stride=stride)\n",
    "    # asserts\n",
    "    out = convolution(image, filt, bias, stride=stride, padding=padding)\n",
    "    print(out_torch)\n",
    "    print('-'*82)\n",
    "    print(out[:10])\n",
    "    assert out_torch.squeeze().shape == out.shape\n",
    "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2. Max Pooling (2 pkt.)\n",
    "Operacja *max pooling* jest analogiczna do zwykłej konwolucji, lecz zamiast operacji mnożenia z zadanym filtrem na każdym fragmencie wejścia wykonywana jest funkcja *max*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(image: torch.tensor, \n",
    "                kernel_size: int, \n",
    "                stride: int = 1, \n",
    "                padding: int = 1):\n",
    "    \"\"\"\n",
    "    :param image: torch.Tensor \n",
    "        Input image of shape (C, H, W)\n",
    "    :param kernel_size: int \n",
    "        Size of the square pooling kernel\n",
    "    :param stride: int\n",
    "        Stride to use in pooling\n",
    "    :param padding: int\n",
    "       Zero-padding to add on all sides of the image \n",
    "    \"\"\"\n",
    "    # get image dimensions\n",
    "    img_channels, img_height, img_width = image.shape\n",
    "    # calculate the dimensions of the output image\n",
    "    out_height = int(((img_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "    out_width = int(((img_width - kernel_size + 2 * padding) / stride) + 1) \n",
    "    out_channels = img_channels \n",
    "\n",
    "    if padding > 0:\n",
    "        padded_height = img_height + 2 * padding\n",
    "        padded_width = img_width + 2 * padding\n",
    "\n",
    "        padded_img = torch.zeros((img_channels, padded_height, padded_width))\n",
    "        padded_img[:, padding:-padding, padding:-padding] = image\n",
    "    else:\n",
    "        padded_height = img_height\n",
    "        padded_width = img_width\n",
    "        padded_img = image\n",
    "\n",
    "    out = torch.zeros((out_channels, out_height, out_width))\n",
    "\n",
    "    for i in range(0, padded_height - kernel_size + 1, stride):\n",
    "\n",
    "        for j in range(0, padded_width - kernel_size + 1, stride):\n",
    "\n",
    "            img_crop = padded_img[:, i:i+kernel_size, j:j+kernel_size]\n",
    "                        \n",
    "            out[:, i // stride, j // stride] = torch.max(torch.max(img_crop, dim=2)[0], dim=1)[0]\n",
    "\n",
    "    return out\n",
    "# image_padded[padding+image_orig.shape[0],padding + image_orig.shape[1]] = image_orig\n",
    "# i:i + filters[x,:,:,:], j:j + filters[x,:,:,:]\n",
    "#     print('image')\n",
    "#     print(image)\n",
    "#     print('filters')\n",
    "#     print(filters)\n",
    "#     print('bias')\n",
    "#     print(bias)\n",
    "#     print('padding')\n",
    "#     print(padding)\n",
    "#     print('stride')\n",
    "#     print(stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Test\n",
    "from itertools import product\n",
    "\n",
    "# cast the frog to tensor\n",
    "image = torch.tensor(np_image)\n",
    "# preapre parameters for testing\n",
    "kernel_sizes = [2, 3, 4]\n",
    "paddings = [0, 1]\n",
    "strides = [1, 2, 3, 4]\n",
    "\n",
    "# test all combinations\n",
    "for kernel_size, stride, padding in product(kernel_sizes, strides, paddings):\n",
    "    # your pooling\n",
    "    out = max_pooling(image, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    # PyTorch equivalent\n",
    "    out_torch = torch.nn.functional.max_pool2d(input=image.unsqueeze(0), kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "    # asserts\n",
    "    assert out_torch.squeeze().shape == out.shape\n",
    "    assert torch.allclose(out, out_torch.squeeze(), atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     f = filters.shape[0]\n",
    "#     whole_output = []\n",
    "#     for filtr in range(f):\n",
    "#         warstwa = []\n",
    "#         for channel in range(img_channels):\n",
    "#             layer_out = []\n",
    "#             felus = filters[filtr][channel]\n",
    "#             for h in range(0, img_height, stride):\n",
    "#                 line_out = []\n",
    "#                 for w in range(0, img_width, stride):\n",
    "#                     mat = image[channel][h:h+filter_size, w:w+filter_size]\n",
    "#                     if mat.shape == (filter_size, filter_size):\n",
    "#                         out = mat * felus\n",
    "#                         out_sum = float(out.sum())\n",
    "#                 if line_out:\n",
    "#                     layer_out.append(line_out)\n",
    "#             warstwa.append(layer_out)\n",
    "#         whole_output.append(warstwa)\n",
    "    \n",
    "#     sub_wynik = torch.tensor(whole_output)\n",
    "#     warstwa_filtra, x, y, z = sub_wynik.shape\n",
    "\n",
    "#     WYNIK = []\n",
    "#     for warstwa in range(warstwa_filtra):\n",
    "#         sub_wynik[warstwa]\n",
    "#         b = bias[warstwa]\n",
    "#         wynik = torch.zeros([y, z], dtype=torch.float64)\n",
    "#         for i in range(x):\n",
    "#             wynik += sub_wynik[warstwa][i]\n",
    "#             #print(f\"wynik: {wynik}\")\n",
    "#         wynik += b\n",
    "#         WYNIK.append(wynik)\n",
    "#     result = []\n",
    "#     for i in WYNIK:\n",
    "#         p = i.tolist()\n",
    "#         result.append(p)\n",
    "#     WYNIK = torch.tensor(result)\n",
    "    \n",
    "#     return WYNIK\n",
    "                    \n",
    "\n",
    "    \n",
    "\n",
    "#     for i in range(n_filters):\n",
    "#         for j in range(img_channels):\n",
    "#             for x in range(out_width):\n",
    "#                 if x > img_width2 - filter_size:\n",
    "#                     break\n",
    "#                 if x % stride == 0:\n",
    "#                     for z in range(out_height):\n",
    "#                         if z > img_height2 - filter_size:\n",
    "#                             break\n",
    "#                         try: \n",
    "#                             biases = torch.full_like(input = filters[i,:,:,:], fill_value = bias[i].item())\n",
    "# #                             filter_toadd = filters[i,:,:,:] + biases\n",
    "#                             gowno = (filters[i,j,:,:] * (image_padded[j,x: x + filter_size, z:z + filter_size])).sum()\n",
    "# #                             print(filters[i,j,:,:])\n",
    "# #                             print(image_padded[j,x: x + filter_size, z:z + filter_size])\n",
    "# #                             print('-'*90)\n",
    "# #                             print(gowno)\n",
    "#                             image_cropped = image_padded[x: x + filter_size, z:z + filter_size]\n",
    "#                             dupa = (filters[i,j,:,:] * (image_padded[j,x: x + filter_size, z:z + filter_size])).sum() + bias[i]\n",
    "# #                             print(dupa)\n",
    "# #                             dupa += bias[i]\n",
    "#                             output[i,x,z] = torch.matmul(filters[i,j,:,:], image_cropped).sum() + bias[i]\n",
    "\n",
    "#                         except Exception as e:\n",
    "#                             exc_info = sys.exc_info()\n",
    "#                             traceback.print_exception(*exc_info)\n",
    "#                             print(f'Coś poszo nie tak: \\n{e}')\n",
    "#                             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
